Robots.txt Experiments and Metrics
==================================

How is the Robots Exclusion Protocol (robots.txt or [RFC 9309](https://datatracker.ietf.org/doc/rfc9309/)) used in the WWW? This projects tries to get some insights mining Common Crawl's robots.txt captures of the years 2016 – 2024.





## Notes and Credits

This project is an extension of work done for a presentation at #ossym2022:
"[The robots.txt standard – Implementations and Usage](https://indico.cern.ch/event/1149330/contributions/5074600/)".
The corresponding code is found at [ossym2022-robotstxt-experiments](https://github.com/sebastian-nagel/ossym2022-robotstxt-experiments).

The idea to look at multiple strata (top-k) is inspired by the work of Longpre et al. "Consent in crisis" (<https://arxiv.org/abs/2407.14933>)
and Liu et al. "Somesite I used to crawl" (<https://arxiv.org/pdf/2411.15091>).
